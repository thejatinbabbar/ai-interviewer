## AI Interviewer

### Overview
**AI Interviewer** is a chatbot-based interview application designed to conduct dynamic interviews and evaluate candidates’ soft skills. The application features a modular design where:
- The **frontend** (built with Streamlit) provides a user-friendly chat interface.
- The **llm** (powered by FastAPI) handles interview logic and dynamic question generation.
- **Artifacts** contain interview data and seeding scripts.
- **Database service (db)** manages transcript storage and evaluation results.

### Features
- **Dynamic Interviewing:** The system generates tailored interview questions based on candidate input.
- **Soft Skills Evaluation:** It assesses candidates on key areas like communication, teamwork, leadership, and adaptability.
- **Transcript Storage:** Interviews and evaluations are logged for later review.
- **Modular Architecture:** Each service is containerized and orchestrated using Docker Compose.

### Project Structure
- **frontend:**  
  - Contains the Streamlit app for candidate interaction (frontend/app.py).
  - Dependencies are listed in requirements.txt.
- **llm:**  
  - Contains the FastAPI backend that manages interview sessions (llm/api.py) and interview logic (llm/interview_chain.py).
  - Configuration for the LLM and vectorstore is provided in config.yml.
- **db:**  
  - Provides a simple FastAPI service to log and retrieve conversation transcripts (db/main.py).
- **artifacts:**  
  - Stores interview data, seeding scripts, and models.
- **Supporting Files:**
  - docker-compose.yml – For local development setup.
  - Makefile – For development commands (formatting, cleaning, installing).
  - ci.yml – CI pipeline configuration.

### Prerequisites
- Docker and Docker Compose
- Python 3.9
- Ollama: This project requires Ollama installed on your machine. Once you select a model, add the model name in the config.yml and run the following command to download the model locally:
  
  ```sh
  ollama pull <model-name>
  ```

### Setup & Local Development

1. **Clone the Repository:**

   ```sh
   git clone <repository-url>
   cd <repository-folder>
   ```

2. **Install Development Dependencies:**

   First, create a virtual environment (e.g. with venv or conda), activate the environment, and run the following command to install formatting tools:
   ```sh
   make install
   ```
   This installs the dependencies listed in requirements files.

3. **Run Locally Using Docker Compose:**

   Build and run the services:
   ```sh
   docker-compose up --build
   ```
   - **llm** service will be available on port 8000.
   - **frontend** (Streamlit) will be available on port 8501.
   - **db-service** will be available on port 8001.

### CI/CD

The CI pipeline (.github/workflows/ci.yml) runs on pushes and pull requests to the `main` branch, ensuring that code formatting is correct and dependencies are installed.

### Usage

1. **Start an Interview:**  
   Access the Streamlit frontend via the assigned port (default: 8501). Enter candidate details to initialize the interview session.
2. **Conduct the Interview:**  
   The frontend displays interview questions generated by the FastAPI backend. Candidate responses drive follow-up questions.
3. **Finish the Interview:**  
   Once the maximum number of questions is reached, the interview concludes, triggering an evaluation step. Transcript and evaluation data is then saved.
4. **Review Logs & Evaluation:**  
   Conversation logs and evaluations are stored in the database managed by the **db/service**.
